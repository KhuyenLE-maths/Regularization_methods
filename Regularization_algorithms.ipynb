{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we aim to compare some regularization methods as Logistic regression, LASSO (Least Absolute Shrinkage and Selection Operator) and Elastic Net. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "Given an observation matrix $H \\in \\mathbb{R}^{n \\times p}$, and a response vector $y \\in \\mathbb{R}^n$, where $n$ and $p$ are the observation and the variable number, respectively. Our purpose is to find the solution of the following optimization problem:  \n",
    "    \\begin{equation} \\label{LS}\n",
    "    u^*_{ls} = \\underset{u}{argmin} \\:  \\| y - Hu \\|^2. \\tag{1}\n",
    "    \\end{equation}\n",
    "\n",
    "If the matrix $H^TH$ is invertible, then we can easily find the solution of (1) by the following formula:\n",
    "    \\begin{equation}  \\label{sol_LS}\n",
    "\tu^*_{ls} = (H^TH)^{-1} H^T y.\n",
    "\t\\end{equation}\n",
    "But in the real datasets, the number of observation number $n$ is usually much smaller than the variable one. In such a case, the matrix $H^TH$ is not invertible. Hence, the above solution is not valid anymore. In order to overcome this drawback, some regularization methods were proposed as Ridge regression, LASSO (Least Absolute Shrinkage and Selection Operator) and Elastic Net. In this blog, we review these algorithms as well as their pros and cons. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Ridge regression ($l_2$-regularization)\n",
    "This technique were first introduced by Arthur Hoerl and Robert Kennard in Technometrics in 1970. The main idea of this method is adding an $l_2$ regularization term $\\lambda \\|u\\|_2^2$ to the least-square problem (1):\n",
    "    \\begin{equation} \\label{RR} \n",
    "\tu^*_{l_2} = \\underset{u}{argmin} \\: \\left ( \\| y - Hu\\|^2 + \\lambda_2 \\| u \\|_2^2 \\right). \\tag{2}\n",
    "\t\\end{equation}\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "Let us denote:\n",
    "\t\\begin{equation}\n",
    "\t\\mathcal{F}_{l_2}(u) = \\| y - Hu\\|^2 + \\lambda_2 \\| u \\|^2. \n",
    "\t\\end{equation}\n",
    "Then, the solution of Problem (2) satisfies the following condition: \n",
    "\t\\begin{equation}\n",
    "\t\\frac{\\partial \\mathcal{F}_{l_2}(u^*_{l_2})}{\\partial u} = 0\t\\Leftrightarrow  2 H^T (H u^*_{l_2} - y) + 2 \\lambda_2 u^*_{l_2} = 0\n",
    "\t\\Leftrightarrow  (H^T H + \\lambda_2 I) u^*_{l_2} = H^T y. \n",
    "\t\\end{equation}\n",
    "\n",
    "Hence, the solution of Problem (2) is given by:\n",
    "\t\\begin{equation}\n",
    "\t\\boxed{u^*_{l_2} = (H^TH + \\lambda_2 I)^{-1} H^T y.}\n",
    "\t\\end{equation}\n",
    "**Remarks**  \n",
    "- Since $rank(H^TH + \\lambda_2 I) = p$ for all matrix $H$, then $(H^TH + \\lambda_2 I)$ is always invertible. \n",
    "- The solution $u^*_{l_2}$ of the $l_2$-regularization problem (2) is proportional to the solution $u^*_{ls}$ of the Least-square problem (1). Especially, $u^*_{l_2}$ is shrunk when $\\lambda_2$ is large. \n",
    "\n",
    "When the variable number is large, we are interested in selecting only the most important ones. Hence we need to find a method that imposes the weak ones to zeros and keep only the strong ones. Since the Ridge regression solutions are only shrunk and non of them equals to zero, then it is not useful for selecting variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LASSO ($l_1$-regularization) problem\n",
    "\n",
    "LASSO (Least Absolute Shrinkage and Selection Operator) was developed by Robert Tibshirani in 1996 (Regression Shrinkage and selection via the lasso). This method was born to deal with the disadvantage of Ridge regression method for variable selection task. In this method, the author proposed to add a constraint on the solution of the least square problem: \n",
    "    \\begin{equation}\n",
    "    u^* = \\underset{u}{argmin} \\: \\left ( \\| y - Hu \\|^2 \\right), \\quad \\text{contraints to} \\quad \\| u \\|_1 \\leq t\n",
    "    \\end{equation}\n",
    "It is equivalent to:\n",
    "    \\begin{equation}\\label{l1}\n",
    "\t\tu^* = \\underset{u}{argmin} \\: \\left ( \\| y - Hu \\|^2 + \\lambda_1 \\| u \\|_1 \\right), \\tag{3}\n",
    "\t\t\\end{equation}\n",
    "where $\\|u\\|_1 = \\sum_{i = 1}^p | u_i |$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Let us denote \n",
    "\t\\begin{equation}\n",
    "\t\\mathcal{F}_{l_1}(u) = \\| y - Hu \\|^2 + \\lambda_1 \\| u \\|_1. \n",
    "\t\\end{equation}\n",
    "Then, the solution of Problem (3) satisfies the following condition: \n",
    "\t\\begin{equation} \\label{condition-l1}\n",
    "\t\\frac{\\partial \\mathcal{F}_{l_1}(u^*_{l_1})}{\\partial u} = 0 \n",
    "\t\\Leftrightarrow 2 H^T (H u^*_ {l_1} - y) + \\lambda_1 \\Lambda = 0, \\tag{3.1}\n",
    "\t\\end{equation}\n",
    "where \n",
    "$\\Lambda_i = sign(u_i) = \\left \\{ \\begin{array}{ll} 1 & \\text{ if } u_i > 0 \\\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  -1 & \\text{ if } u_i < 0 \\\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t u_0 \\in [-1, 1] & \\text{ if } u_i = 0\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\\end{array} \\right. $\n",
    "\n",
    "When $H$ is an orthogonal matrix (i.e $H^T H = I$), we can find the exact value of $u^*_{l_1}$. Indeed, Equation (3.1) can be rewritten as:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\t& 2 u^*_{l_1} - 2 H^T y + \\lambda_1 \\Lambda = 0, \\\\\n",
    "\t\\Leftrightarrow \\: & 2 (u^*_{l_1})_j - 2 H^T_j y + \\lambda_1 sign((u^*_{l_1})_j) = 0,\\quad \\forall j = \\overline{1,p}, \\\\\n",
    "\t\\Leftrightarrow \\:  & 2 |(u^*_{l_1})_j| sign((u^*_{l_1})_j) + \\lambda_1 sign ((u^*_{l_1})_j) = 2 H^T_j y,  \\quad \\forall j = \\overline{1,p}, \\\\ \n",
    "\t\\Leftrightarrow \\: & (2 | (u^*_{l_1})_j | + \\lambda_1) sign((u^*_{l_1})_j) = 2 | H^T_j y | sign(H^T_j y), \\quad \\forall j = \\overline{1,p}.\n",
    "\t\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means that $ sign((u^*_{l_1})_j) = sign(H^T_j y), \\forall j = \\overline{1,p}$. \\\\\n",
    "In the other hand, we have\n",
    "\t\\begin{equation}\n",
    "\t2 (u^*_{l_1})_j = 2 H^T_j y - \\lambda_1 sign((u^*_{l_1})_j), \\forall j = \\overline{1,p}.\n",
    "\t\\end{equation}\n",
    "Hence, \n",
    "\t\\begin{equation}\n",
    "\t(u^*_{l_1})_j = \\left ( |H^T_j y| - \\frac{\\lambda_1}{2} \\right) sign((u^*_{l_1})_j), \\forall j = \\overline{1,p}.\n",
    "\t\\end{equation}\n",
    "\n",
    "- If $(u^*_{l_1})_j >0$, then $sign ((u^*_{l_1})_j) = 1$. Hence, $(u^*_{l_1})_j = |H^T_j y| - \\dfrac{\\lambda_1}{2} > 0$, (i.e $|H^T_j y| > \\dfrac{\\lambda_1}{2}$).\n",
    "- If $(u^*_{l_1})_j <0$, then $sign ((u^*_{l_1})_j) = -1$. Hence, $(u^*_{l_1})_j = - |H^T_j y| + \\dfrac{\\lambda_1}{2} < 0$, (i.e $|H^T_j y| > \\dfrac{\\lambda_1}{2}$).\n",
    "- Otherwise, $(u^*_{l_1})_j = 0$. \n",
    "\n",
    "In conclusion, the general solution of Problem (3) (when $H$ is orthogonal) is given by:\n",
    "\t\\begin{equation} \n",
    "\t\\boxed{(u^*_{l_1})_j = \\max \\left ( |H^T_j y| - \\frac{\\lambda_1}{2}, 0 \\right). sign(H^T_j y), \\: \\forall j = \\overline{1,p}.} \\tag{3.2}\n",
    "\t\\end{equation}\n",
    "\n",
    "**Comments**\n",
    "\n",
    "- The estimated coefficient $(u^*_{l_1})_j$ is different from 0 if and only if $|H^T_j y| > \\dfrac{\\lambda_1}{2}$. In the other words, if $\\lambda_1 \\geq 2 |H^T_j y|$, then $(u^*_{l_1})_j = 0$. Therefore, we can control the sparsity level of the solution based on the values of $|H^T_j y |$. \n",
    "- The solution given by Eq. (3.2) is sparse. Its sparsity level depends on the regularization parameter $\\lambda_1$: the larger $\\lambda_1$ is, the sparser is the solution. \n",
    "\n",
    "**Compare the solutions of $l_1$-regularization (LASSO) problem (3) and the solution of the $l_2$-regularization (Ridge regression) problem (2)}**\n",
    "\n",
    "- The LASSO solution (3.2) is sparse, while the ridge regression solution is only shrunk and all coefficients are non-zeros (when $\\lambda_2 < + \\infty$). When the variable number is large, we are interested in finding a subset of variables which have a significance predictive power, (i.e we aim to select the most important variables). By setting some coefficients to 0, the LASSO solution ignores the less useful ones, and only keeps the important ones. That makes sense for interpretation variables as well as for reducing feature dimension in high dimensional cases. \n",
    "- However, the advantage of the LASSO solution is also its drawback when the regularization parameter $\\lambda_1$ is too large. In this case, the estimation is too sparse, so we may loss the signal information in the rejected variables. This limitation of LASSO is an inspiration for the introduction of the Elastic-net method which will be presented in the following section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Elastic-net ($l_1$ and $l_2$ - regularization) problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Elastic-net method was proposed by Hui Zou and Trevor Hastie in 2005. The authors proposed to deal with the limitation of LASSO by adding a quadratic regularization term $\\lambda_2 \\| u \\|_2^2$ to the LASSO problem:\n",
    "\\begin{equation}\n",
    "u^*_{el} = \\underset{u}{argmin} \\: ( \\| y - Hu\\|^2 + \\lambda_1 \\|u \\|_1 + \\lambda_2 \\|u \\|_2^2), \\tag{4}\n",
    "\\end{equation}\n",
    "where $y \\in \\mathbb{R}^n, H \\in \\mathbb{R}^{n \\times p}, u \\in \\mathbb{R}^p$, $\\|u \\|_1 = \\sum_{i = 1}^p |u_i|$, $\\|u \\|_2^2 = \\sum_{ i = 1}^p u_i^2$. \n",
    "\n",
    "**Solution:**\n",
    "\n",
    "In order to understand how this method overcome the limitation of LASSO, let's try to find out its solution: \n",
    "\n",
    "Denote \n",
    "\t\t\\begin{equation}\n",
    "\t\t\\mathcal{F}_{el} = \\| y - Hu\\|^2 + \\lambda_1 \\|u \\|_1 + \\lambda_2 \\|u \\|_2^2. \n",
    "\t\t\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting\n",
    "\\begin{equation}\n",
    "\t\\begin{array}{ll}\n",
    "\ty' = \\begin{pmatrix} y \\\\ 0 \\end{pmatrix}  \\in \\mathbb{R}^{n + p} \n",
    "\t& H' = \\dfrac{1}{\\sqrt{1 + \\lambda_2}} \\begin{pmatrix} H \\\\ \\sqrt{\\lambda_2} I \\end{pmatrix} \\in \\mathbb{R}^{(n+p)\\times p} \\\\\n",
    "\t\\lambda' = \\dfrac{\\lambda_1}{\\sqrt{1 + \\lambda_2}} \\in \\mathbb{R}^+\n",
    "\t& u' = \\sqrt{1 + \\lambda_2} u \\in \\mathbb{R}^p. \n",
    "\t\\end{array}\n",
    "\t\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\mathcal{F}_{el}$ can be represented by:\n",
    "\t\\begin{align*}\n",
    "\t\\mathcal{F}_{el} & = \\| y - Hu\\|^2 + \\lambda_2 \\|u \\|_2^2 + \\lambda_1 \\|u \\|_1, \\\\\n",
    "\t& = \\begin{Vmatrix} y - Hu \\\\ 0 - \\sqrt{\\lambda_2}u \\end{Vmatrix}^2 + \\lambda_1 \\|u\\|_1, \\\\\n",
    "\t& = \\begin{Vmatrix} \\begin{pmatrix} y \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} H \\\\ \\sqrt{\\lambda_2} I \\end{pmatrix} u \\end{Vmatrix}^2 + \\lambda_1 \\|u\\|_1, \\\\\n",
    "\t& = \\| y' - H' u' \\|^2 + \\lambda' \\| u'\\|_1 := \\mathcal{F}_{l_1}(u'). \n",
    "\t\\end{align*}\n",
    "\n",
    "Therefore, the solution of the Elastic-net problem (4) is given by: \n",
    "\t\\begin{equation} \\label{sol_EL}\n",
    "\t\\boxed{u^*_{el} = \\dfrac{1}{\\sqrt{1 + \\lambda_2}} u'_{l_1-reg}}\n",
    "\t\\end{equation}\n",
    "where $u'_{l_1-reg}$ is the solution of the $l_1$-regularization problem: \n",
    "\t\\begin{equation}\n",
    "\tu'_{l_1-reg} = \\underset{u'}{argmin} \\: \\left ( \\| y' - H' u' \\|^2 + \\lambda' \\| u'\\|_1 \\right).\n",
    "\t\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the solution of the elastic-net problem to the solution of the $l_1$-regularization (LASSO) and the solution of the $l_2$-regularization (Ridge regression) problem:**\n",
    "\n",
    "As I presented in the previous sections that the ridge regression solution selects all variables, while the LASSO solution selects only the most important ones by setting the less powerful variables to zeros. That makes the LASSO solution become more helpful for predicting as well as for interpreting variables in high dimensional case. However, the LASSO solution still has some limitations. When the variable number $p$ is much larger than the observation number $n$, LASSO selects at most $n$ variables, before it saturates. (Indeed, based on the first comment in Section 2, we have $(u^*_{l_1})_j \\neq 0$ if and only if $2 |H^T_j y| > \\lambda_1$. Since $H \\in \\mathbb{R}^{n \\times p}, p \\gg n$, then $rank(H) \\leq n$. Hence, we have at most $n$ variables which are different to 0 in the LASSO solution). As $p \\gg n$, we may loss the information of other $(p-n)$ variables which are not selected. Besides, when $n >p$, if the variables are highly correlated, the prediction accuracy of the $l_2$-regularization method is higher than the $l_1$-regularization. Therefore, the combination of $l_1$ and $l_2$ regularization aim to use the advantage of the $l_2$ (when the variables are highly correlated) to overcome the drawback on $l_1$-regularization method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain:**\n",
    "We have proved that the solution of Elastic-net problem (4) is given by: \n",
    "\t\\begin{equation}\n",
    "\tu^*_{el} = \\dfrac{1}{\\sqrt{1 + \\lambda_2}} u'_{l_1-reg},\n",
    "\t\\end{equation}\n",
    "where $u'_{l_1-reg}$ is the solution of the $l_1$-regularization problem: \n",
    "\t\\begin{equation}\n",
    "\tu'_{l_1-reg} = \\underset{u'}{argmin} \\: \\left ( \\| y' - H' u' \\|^2 + \\lambda' \\| u'\\|_1 \\right).\n",
    "\t\\end{equation}\n",
    "From the above demonstration, we have $(u'_{l_1-reg})_j \\neq 0$ if and only if $|(H')^T_j y'| > 2 \\lambda'$. Besides, $H' \\in \\mathbb{R}^{(n+p) \\times p}$, hence $n < rank(H') \\leq p$. That means that the selected variable number can be reached to $p$ when $p \\gg n$. It demonstrates that the Elastic-net solution overcomes the drawback the the LASSO solution in the problem of variable selection. \n",
    "    \n",
    "Disadvantage of Elastic-net: For this problem, we have to compute in the matrix of $((p+n) \\times p)$-dimension. When we have a million (or more than a million) of variables, the computational amount is enormous. So this method is not really useful in high dimensional case. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "[1] Hoerl, Arthur E., and Robert W. Kennard. \"Ridge regression: Biased estimation for nonorthogonal problems.\" Technometrics 12.1 (1970): 55-67.\n",
    "\n",
    "[2] Tibshirani, Robert. \"Regression shrinkage and selection via   the lasso.\" Journal of the Royal Statistical Society: Series B (Methodological) 58.1 (1996): 267-288.\n",
    "\n",
    "[3] Zou, Hui, and Trevor Hastie. \"Regularization and variable selection via the elastic net.\" Journal of the royal statistical society: series B (statistical methodology) 67.2 (2005): 301-320."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
